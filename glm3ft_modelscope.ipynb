{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17618990-983b-449a-a048-f806b7697af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T02:12:55.978343Z",
     "iopub.status.busy": "2024-05-03T02:12:55.978051Z",
     "iopub.status.idle": "2024-05-03T02:12:56.479211Z",
     "shell.execute_reply": "2024-05-03T02:12:56.478576Z",
     "shell.execute_reply.started": "2024-05-03T02:12:55.978323Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace\n",
      "glm3ft.ipynb  ZhipuAI-chatglm3-6b.ipynb\n",
      "/\n",
      "'=0.21.0'                     \u001b[0m\u001b[01;34mdev\u001b[0m/     \u001b[01;36mlibx32\u001b[0m@                    \u001b[01;34mproc\u001b[0m/   \u001b[34;42mtmp\u001b[0m/\n",
      "'=0.6.0'                      \u001b[01;34metc\u001b[0m/     \u001b[01;34mmedia\u001b[0m/                     \u001b[01;34mroot\u001b[0m/   \u001b[01;34musr\u001b[0m/\n",
      "'=2.25.0'                     \u001b[01;34mhome\u001b[0m/    \u001b[01;34mmnt\u001b[0m/                       \u001b[01;34mrun\u001b[0m/    \u001b[01;34mvar\u001b[0m/\n",
      " \u001b[01;36mbin\u001b[0m@                         \u001b[01;36mlib\u001b[0m@     \u001b[01;34mmodelscope\u001b[0m/                \u001b[01;36msbin\u001b[0m@\n",
      " \u001b[01;34mboot\u001b[0m/                        \u001b[01;36mlib32\u001b[0m@   NGC-DL-CONTAINER-LICENSE   \u001b[01;34msrv\u001b[0m/\n",
      " cuda-keyring_1.0-1_all.deb   \u001b[01;36mlib64\u001b[0m@   \u001b[01;34mopt\u001b[0m/                       \u001b[01;34msys\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%ls\n",
    "%cd /\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294acbf4-71f6-4f45-acab-e21567bb2053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T07:28:47.459946Z",
     "iopub.status.busy": "2024-05-03T07:28:47.459637Z",
     "iopub.status.idle": "2024-05-03T07:28:47.778406Z",
     "shell.execute_reply": "2024-05-03T07:28:47.777851Z",
     "shell.execute_reply.started": "2024-05-03T07:28:47.459926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 15:28:47 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          On   | 00000000:00:07.0 Off |                    0 |\n",
      "|  0%   33C    P8    22W / 150W |      0MiB / 22731MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70eb1d8-ee55-4622-9551-a3997032467b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting accelerate>=0.27.2\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/53/fe/0251ccd9e0015c705e772da0fb2c96cdafd87b1d7dd45dc13dca7ced0eb7/accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (2.1.2+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.27.2) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2) (2.1.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate>=0.27.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate>=0.27.2) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.27.2) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate>=0.27.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate>=0.27.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate>=0.27.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate>=0.27.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.27.2) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.26.1\n",
      "    Uninstalling accelerate-0.26.1:\n",
      "      Successfully uninstalled accelerate-0.26.1\n",
      "Successfully installed accelerate-0.29.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.27.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8aac0-eed9-4c93-9497-876826691e11",
   "metadata": {},
   "source": [
    "# 下载模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a9ff27-6c89-4f02-a9a3-d0ac2c7c2f24",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:59:03,073 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-05-07 17:59:03,077 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-05-07 17:59:03,077 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-05-07 17:59:03,078 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-05-07 17:59:03,153 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 11536c73f9a69de5892dd60259abcf6a and a total number of 976 components indexed\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-07 17:59:04,955 - modelscope - INFO - Development mode use revision: master\n",
      "Downloading: 100%|██████████| 1.29k/1.29k [00:00<00:00, 5.06MB/s]\n",
      "Downloading: 100%|██████████| 37.0/37.0 [00:00<00:00, 162kB/s]\n",
      "Downloading: 100%|██████████| 2.28k/2.28k [00:00<00:00, 10.7MB/s]\n",
      "Downloading: 100%|█████████▉| 1.70G/1.70G [00:07<00:00, 257MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:07<00:00, 258MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:07<00:00, 263MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [00:07<00:00, 228MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:08<00:00, 225MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:07<00:00, 252MB/s]\n",
      "Downloading: 100%|█████████▉| 0.98G/0.98G [00:05<00:00, 184MB/s]\n",
      "Downloading: 100%|██████████| 20.8k/20.8k [00:00<00:00, 31.8MB/s]\n",
      "Downloading: 100%|██████████| 4.04k/4.04k [00:00<00:00, 14.8MB/s]\n",
      "Downloading: 100%|██████████| 54.6k/54.6k [00:00<00:00, 115MB/s]\n",
      "Downloading: 100%|█████████▉| 1.70G/1.70G [00:07<00:00, 260MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:18<00:00, 109MB/s] \n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:07<00:00, 251MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [00:08<00:00, 218MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:09<00:00, 210MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:08<00:00, 225MB/s]\n",
      "Downloading: 100%|█████████▉| 0.98G/0.98G [00:04<00:00, 243MB/s]\n",
      "Downloading: 100%|██████████| 20.0k/20.0k [00:00<00:00, 29.6MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 22.2MB/s]\n",
      "Downloading: 100%|██████████| 4.73k/4.73k [00:00<00:00, 3.60MB/s]\n",
      "Downloading: 100%|██████████| 3.00/3.00 [00:00<00:00, 13.7kB/s]\n",
      "Downloading: 100%|██████████| 12.7k/12.7k [00:00<00:00, 17.6kB/s]\n",
      "Downloading: 100%|██████████| 995k/995k [00:00<00:00, 24.7MB/s]\n",
      "Downloading: 100%|██████████| 1.36k/1.36k [00:00<00:00, 7.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"ZhipuAI/chatglm3-6b\", cache_dir='chatglm3-6b', revision = \"master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c705f-7d0f-45aa-b61e-8c2bf62174c0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 10953, done.\u001b[K\n",
      "remote: Counting objects: 100% (1770/1770), done.\u001b[K\n",
      "remote: Compressing objects: 100% (428/428), done.\u001b[K\n",
      "remote: Total 10953 (delta 1436), reused 1573 (delta 1332), pack-reused 9183\u001b[K\n",
      "接收对象中: 100% (10953/10953), 214.56 MiB | 11.33 MiB/s, 完成.\n",
      "处理 delta 中: 100% (8035/8035), 完成.\n",
      "正在更新文件: 100% (216/216), 完成.\n",
      "正克隆到 'ft_data'...\n",
      "remote: Enumerating objects: 449, done.\u001b[K\n",
      "remote: Counting objects: 100% (218/218), done.\u001b[K\n",
      "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
      "接收对象中:   6% (27/449)\r"
     ]
    }
   ],
   "source": [
    "!git config --global http.postBuffer 524288000 && git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!git config --global http.postBuffer 524288000 && git clone https://github.com/kangtsang/ft_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b42d83b-c00e-4fcf-b061-c011f23e4136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T01:58:15.803649Z",
     "iopub.status.busy": "2024-05-06T01:58:15.803240Z",
     "iopub.status.idle": "2024-05-06T01:58:15.997706Z",
     "shell.execute_reply": "2024-05-06T01:58:15.997040Z",
     "shell.execute_reply.started": "2024-05-06T01:58:15.803625Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/LLaMA-Factory\n",
      "切换到一个新分支 'master'\n"
     ]
    }
   ],
   "source": [
    "!cd LLaMA-Factory && pwd && git checkout -b 'master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a40764-a843-40b4-931a-dd8f0437ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as tr\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(f\"tf cuda available：{tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# 输出带CPU，表示torch是CPU版本的，否则会是+cuxxx\n",
    "print(f'torch的版本是：{torch.__version__}')\n",
    "print(f'torch是否能使用cuda：{torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a398cb6-3b88-4b1d-a8d8-d531bf7e698e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd LLaMA-Factory && pwd && pip install -e .[metrics,vllm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292e24e-aa0b-4f0f-8b5d-2144a9a9a41d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python LLaMA-Factory/src/train.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b \\\n",
    "    --quantization_bit 8 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --dataset_dir ft_data \\\n",
    "    --dataset ad_gen \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 35 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/glm3_train_2024-05-03 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target query_key_value \\\n",
    "    --plot_loss True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa02938-b111-4ddb-adcd-47f27123afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace\n",
      "/mnt/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# /mnt/workspace\n",
    "!cd LLaMA-Factory   \n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b980d51-4009-4c95-a5b8-237db4e773e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型对话 /mnt/workspace/chatglm3-6b\n",
    "from llmtuner.chat import ChatModel\n",
    "from llmtuner.extras.misc import torch_gc\n",
    "chat_model = ChatModel(dict(\n",
    "  model_name_or_path=\"/mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b\",     # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "  adapter_name_or_path=\"ft_data/train/glm3_train_classify_2024-05-06\",   # 加载之前保存的 LoRA 适配器\n",
    "  finetuning_type=\"lora\",                  # 和训练保持一致\n",
    "  template=\"chatglm3\",                     # 和训练保持一致\n",
    "  # quantization_bit=8,           # 加载 4 比特量化模型\n",
    "))\n",
    "messages = []\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})     # 把提示词添加到消息中\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):      # 流式输出\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response}) # 把回答添加到消息中\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e1830-5f68-4b41-aaa5-15a8a69a84b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fc8df-6b36-4d61-a5e7-af0de2fc5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-07 18:08:49.800286: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 18:08:49.847344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-07 18:08:49.847378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-07 18:08:49.847418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-07 18:08:49.855844: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 18:08:49.856131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 18:08:51.042778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-05-07 18:08:55,977] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "05/07/2024 18:08:57 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-07 18:08:57,008 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-07 18:08:57,008 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-07 18:08:57,008 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-07 18:08:57,008 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-07 18:08:57,008 >> loading file tokenizer.json\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "05/07/2024 18:08:57 - INFO - llmtuner.data.template - Add <|user|>,<|observation|> to stop words.\n",
      "05/07/2024 18:08:57 - INFO - llmtuner.data.template - Cannot add this chat template to tokenizer.\n",
      "05/07/2024 18:08:57 - INFO - llmtuner.data.loader - Loading dataset classify/dev.json...\n",
      "05/07/2024 18:08:57 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Running tokenizer on dataset: 100%|███| 989/989 [00:01<00:00, 717.73 examples/s]\n",
      "input_ids:\n",
      "[64790, 64792, 64795, 30910, 13, 30910, 13, 352, 42693, 30989, 35276, 31793, 30943, 30940, 30943, 30940, 51733, 55773, 53444, 33428, 41925, 54771, 31123, 33575, 48159, 48875, 31950, 40455, 33031, 43404, 50113, 32235, 53010, 33182, 34711, 38724, 32184, 30987, 13, 352, 30938, 30954, 42594, 31845, 31707, 30932, 33302, 33572, 33820, 30932, 42594, 33624, 30932, 30910, 54675, 54593, 33624, 30932, 30910, 46844, 30932, 30910, 32750, 32675, 30932, 30910, 32179, 32675, 30932, 42594, 37670, 32438, 30932, 30910, 32157, 53300, 54609, 30930, 13, 352, 30949, 30954, 42594, 32354, 31707, 30932, 33302, 32354, 33292, 30932, 30910, 32354, 31720, 30932, 30910, 32354, 38724, 30932, 30910, 32354, 31634, 32364, 54609, 30930, 13, 352, 30942, 30954, 30910, 33425, 43978, 31724, 31795, 30932, 30910, 33302, 32457, 39748, 54670, 30932, 30910, 34918, 34134, 54670, 30932, 30910, 33469, 54670, 34001, 35134, 54952, 55005, 30932, 30910, 31779, 32974, 30932, 30910, 32457, 31123, 34309, 31123, 32175, 54609, 30930, 13, 352, 30952, 30954, 30910, 32269, 54736, 30932, 32067, 54708, 52798, 54538, 31921, 31823, 30932, 31665, 31793, 32269, 40384, 31823, 30932, 30910, 31779, 49829, 30932, 30910, 55087, 30932, 30910, 50464, 30932, 30910, 39363, 30932, 34673, 54609, 30930, 30910, 13, 352, 30950, 30954, 30910, 32665, 54736, 31123, 31665, 54708, 37234, 34073, 45843, 31993, 31123, 54534, 31786, 54823, 30967, 36954, 54538, 31636, 45843, 31201, 39535, 31201, 44086, 54585, 31823, 31951, 30930, 352, 13, 352, 30960, 30954, 30910, 32245, 51091, 30932, 31779, 32025, 31689, 30932, 32025, 31847, 30932, 31895, 31689, 30932, 31895, 31731, 30932, 35318, 10954, 31017, 30930, 13, 352, 54622, 37812, 33287, 41260, 39715, 30932, 30910, 31844, 33287, 41260, 39715, 54643, 38684, 36704, 33718, 31722, 31795, 30930, 13, 352, 13, 35276, 31793, 30943, 30940, 30943, 30940, 51733, 55773, 53444, 33428, 41925, 54771, 31123, 33575, 48159, 48875, 31950, 40455, 33031, 43404, 50113, 31514, 64796]\n",
      "inputs:\n",
      "[gMASK] sop <|user|> \n",
      " \n",
      "        请问“能否根据2020年金宇生物技术股份有限公司的年报，给我简要介绍一下报告期内公司的社会责任工作情况？”是属于下面哪个类别的问题?\n",
      "        A: 公司基本信息,包含股票简称, 公司名称, 外文名称, 法定代表人, 注册地址, 办公地址, 公司网址网站, 电子信箱等.\n",
      "        B: 公司员工信息,包含员工人数, 员工专业, 员工类别, 员工教育程度等.\n",
      "        C: 财务报表相关内容, 包含资产负债表, 现金流量表, 利润表 中存在的字段, 包括费用, 资产，金额，收入等.\n",
      "        D: 计算题,无法从年报中直接获得,需要根据计算公式获得, 包括增长率, 率, 比率, 比重,占比等. \n",
      "        E: 统计题，需要从题目获取检索条件，在数据集/数据库中进行检索、过滤、排序后获得结果.        \n",
      "        F: 开放性问题,包括介绍情况,介绍方法,分析情况,分析影响,什么是XXX.\n",
      "        你只需要回答字母编号, 不要回答字母编号及选项文本外的其他内容.\n",
      "        \n",
      "能否根据2020年金宇生物技术股份有限公司的年报，给我简要介绍一下报告期内公司的社会责任工作情况？ <|assistant|>\n",
      "[INFO|configuration_utils.py:724] 2024-05-07 18:08:59,941 >> loading configuration file /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b/config.json\n",
      "[INFO|configuration_utils.py:724] 2024-05-07 18:08:59,942 >> loading configuration file /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-07 18:08:59,943 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "05/07/2024 18:08:59 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3426] 2024-05-07 18:08:59,980 >> loading weights file /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-07 18:08:59,981 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-07 18:08:59,982 >> Generate config GenerationConfig {\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:06<00:00,  1.09it/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-07 18:09:06,486 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-07 18:09:06,487 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:3719] 2024-05-07 18:09:06,490 >> Generation config file not found, using a generation config created from the model config.\n",
      "05/07/2024 18:09:06 - INFO - llmtuner.model.utils.attention - Using vanilla Attention implementation.\n",
      "05/07/2024 18:09:06 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/07/2024 18:09:07 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).\n",
      "05/07/2024 18:09:07 - INFO - llmtuner.model.adapter - Loaded adapter(s): /mnt/workspace/ft_data/train/glm3_train_classify_2024-05-06\n",
      "05/07/2024 18:09:07 - INFO - llmtuner.model.loader - all params: 6243584000\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:3614] 2024-05-07 18:09:07,227 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3616] 2024-05-07 18:09:07,227 >>   Num examples = 989\n",
      "[INFO|trainer.py:3619] 2024-05-07 18:09:07,227 >>   Batch size = 8\n",
      " 19%|███████▊                                  | 23/124 [03:40<17:04, 10.14s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python LLaMA-Factory/src/train.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b \\\n",
    "    --adapter_name_or_path /mnt/workspace/ft_data/train/glm3_train_classify_2024-05-06 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir ft_data \\\n",
    "    --dataset classify_dev \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 128 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.1 \\\n",
    "    --output_dir saves/glm3_keyword_dev_2024-05-06 \\\n",
    "    --do_predict True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e52098-6990-4e36-9276-90c8fb48595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python LLaMA-Factory/src/train.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path /mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b \\\n",
    "    --adapter_name_or_path /mnt/workspace/ft_data\\train\\glm3_train_keyword_2024-05-06 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir ft_data \\\n",
    "    --dataset classify_dev \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 128 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.1 \\\n",
    "    --output_dir saves/glm3_keyword_dev_2024-05-06 \\\n",
    "    --do_predict True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755c871-86c9-4359-83ba-e4a6b7932a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbc4ab-0a59-4b44-bec4-0098512f8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -q -r \"saves_2024-05-07.zip\" /mnt/workspace/saves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
