{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17618990-983b-449a-a048-f806b7697af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "%ls\n",
    "%cd /\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294acbf4-71f6-4f45-acab-e21567bb2053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829fc03-584b-4cd7-880d-63a83212e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as tr\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(f\"tf cuda available：{tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# 输出带CPU，表示torch是CPU版本的，否则会是+cuxxx\n",
    "print(f'torch的版本是：{torch.__version__}')\n",
    "print(f'torch是否能使用cuda：{torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eb1d8-ee55-4622-9551-a3997032467b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install 'accelerate>=0.27.2' modelscope "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8aac0-eed9-4c93-9497-876826691e11",
   "metadata": {},
   "source": [
    "# 下载模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9ff27-6c89-4f02-a9a3-d0ac2c7c2f24",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"ZhipuAI/chatglm3-6b\", cache_dir='chatglm3-6b', revision = \"master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c705f-7d0f-45aa-b61e-8c2bf62174c0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git config --global http.postBuffer 524288000 && git clone https://github.com/hiyouga/LLaMA-Factory.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870aaeb-7198-4c47-8aa3-5f1da9ffac0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git config --global http.postBuffer 524288000 && git clone https://github.com/kangtsang/ft_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42d83b-c00e-4fcf-b061-c011f23e4136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd LLaMA-Factory && pwd && git checkout -b 'master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a8959-e9b6-4891-8fc2-d5fc00fdb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd LLaMA-Factory && pwd && pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a398cb6-3b88-4b1d-a8d8-d531bf7e698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd LLaMA-Factory && pwd && pip install -e .[metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defd850-933a-4ba0-aabc-07bf106a4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练问题分类 需要修改输出保存目录 output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292e24e-aa0b-4f0f-8b5d-2144a9a9a41d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python LLaMA-Factory/src/train.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /hy-tmp/chatglm3-6b/ZhipuAI/chatglm3-6b \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --dataset_dir ft_data \\\n",
    "    --dataset classify \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 35 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 2 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/glm3_train_classify_2024-05-06 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target query_key_value \\\n",
    "    --plot_loss True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031f8fc-b745-47f7-ac46-ea976790bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练关键词提取 需要修改输出保存目录 output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b09654-c2d3-4752-951b-ea52ab539be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python LLaMA-Factory/src/train.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /hy-tmp/chatglm3-6b/ZhipuAI/chatglm3-6b \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --dataset_dir ft_data \\\n",
    "    --dataset keyword \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 35 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 2 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/glm3_train_keyword_2024-05-06 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target query_key_value \\\n",
    "    --plot_loss True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfeb61-d4d3-4bd7-95e2-165061209392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac128b3-d162-4c78-8cfb-1d611a4592bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩训练输出结果 保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa02938-b111-4ddb-adcd-47f27123afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -q -r \"saves_2024-05-06.zip\" /hy-tmp/saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b980d51-4009-4c95-a5b8-237db4e773e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型对话 /mnt/workspace/chatglm3-6b\n",
    "\n",
    "from llmtuner import ChatModel\n",
    "from llmtuner.extras.misc import torch_gc\n",
    "chat_model = ChatModel(dict(\n",
    "  model_name_or_path=\"/mnt/workspace/chatglm3-6b/ZhipuAI/chatglm3-6b\",     # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "  adapter_name_or_path=\"saves/glm3_train_2024-05-03\",   # 加载之前保存的 LoRA 适配器\n",
    "  finetuning_type=\"lora\",                  # 和训练保持一致\n",
    "  template=\"chatglm3\",                     # 和训练保持一致\n",
    "  quantization_bit=8,                      # 加载 4 比特量化模型\n",
    "))\n",
    "messages = []\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})     # 把提示词添加到消息中\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):      # 流式输出\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response}) # 把回答添加到消息中\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e134e6-0fa0-4bff-a52c-d65070978a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
